{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/serge-m/pytorch-nn-tools.git@master\n",
    "# !pip install pytorch-nn-tools==0.3.7\n",
    "# !pip install torch_lr_finder==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U albumentations==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Callable, Union\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "from pytorch_nn_tools.visual import ImgShow, tfm_vis_img, UnNormalize_, imagenet_stats\n",
    "from pytorch_nn_tools.train.metrics.processor import mod_name_train, mod_name_val, Marker\n",
    "from pytorch_nn_tools.train.metrics.processor import MetricAggregator, TensorBoardMetricLogger\n",
    "from pytorch_nn_tools.metrics.accuracy import topk_accuracy\n",
    "from pytorch_nn_tools.train.progress import ProgressTracker\n",
    "from pytorch_nn_tools.convert import map_dict\n",
    "from pytorch_nn_tools.train.metrics.history_condition import HistoryCondition\n",
    "from pytorch_nn_tools.devices import to_device\n",
    "import ml_dataset_tools as mdt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.trainer_io import TrainerIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ish = ImgShow(ax=plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "\n",
    "batch_size_train, batch_size_val, device = 2, 2, 'cpu'\n",
    "batch_size_train, batch_size_val, device = 128, 128, 'cuda'\n",
    "\n",
    "data_root_path = Path(\"data/\")\n",
    "data_path = data_root_path.joinpath(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "size_h_w = 224, 224\n",
    "\n",
    "imagenet_stats = dict(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "cifar_stats = dict(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**cifar_stats),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**cifar_stats),\n",
    "])\n",
    "\n",
    "\n",
    "ds_tr  = datasets.CIFAR10(root=data_root_path, train=True, download=True, transform=transform_train)\n",
    "ds_val = datasets.CIFAR10(root=data_root_path, train=False, download=False, transform=transform_test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=ds_tr,\n",
    "        batch_size=batch_size_train,\n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    \n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=ds_val,\n",
    "        batch_size=batch_size_val,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_images(tb_writer, images, iteration_id):\n",
    "    with torch.no_grad():\n",
    "        vis = images.detach().clone()\n",
    "        for v in vis:\n",
    "            v[:] = UnNormalize_(**cifar_stats)(v)\n",
    "        grid = torchvision.utils.make_grid(vis)\n",
    "        tb_writer.add_image('images', grid, iteration_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, device, trainer_io: TrainerIO,\n",
    "                continue_training: bool = False):\n",
    "        self.device = device\n",
    "        self.continue_training = continue_training\n",
    "        self.trainer_io = trainer_io\n",
    "        \n",
    "    def fit(self, model, optimizer, scheduler, start_epoch, end_epoch, train_dataloader, val_dataloader):\n",
    "        metric_logger = TensorBoardMetricLogger(self.trainer_io.tb_summary_writer)\n",
    "        model = to_device(model, self.device)\n",
    "        \n",
    "        if self.continue_training:\n",
    "            start_epoch = self.trainer_io.load_last(start_epoch, end_epoch, model, optimizer, scheduler)\n",
    "\n",
    "        progr_train = ProgressTracker()\n",
    "        \n",
    "        for epoch in self.trainer_io.main_progress_bar(range(start_epoch, end_epoch)):\n",
    "            metric_aggregator = MetricAggregator()\n",
    "            self.train_epoch(\n",
    "                train_dataloader, progr_train,\n",
    "                model, optimizer, scheduler,  \n",
    "                metric_proc=mod_name_train+metric_aggregator+metric_logger,\n",
    "                report_step=100,\n",
    "            )\n",
    "            self.validate_epoch(\n",
    "                val_dataloader,\n",
    "                model,  \n",
    "                metric_proc=mod_name_val+metric_aggregator+metric_logger,\n",
    "            )\n",
    "            \n",
    "            aggregated = map_dict(metric_aggregator.aggregate(), key_fn=lambda key: f\"avg.{key}\")\n",
    "            metric_logger({\n",
    "                **aggregated, \n",
    "                **{f\"lr_{i}\": lr for i, lr in enumerate(scheduler.get_last_lr())},\n",
    "                Marker.EPOCH: epoch,\n",
    "            })\n",
    "            self.trainer_io.set_main_status_msg(f\"{aggregated}\")\n",
    "            self.trainer_io.save_checkpoint(aggregated, model, optimizer, scheduler, epoch)\n",
    "            \n",
    "        metric_logger.close()\n",
    "            \n",
    "    def train_epoch(self, data_loader, progr, model, optimizer, scheduler, metric_proc, report_step):\n",
    "        model.train()\n",
    "                \n",
    "        for batch in self.trainer_io.secondary_progress_bar(progr.track(data_loader)):\n",
    "            batch = to_device(batch, self.device)\n",
    "            images, target = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "                        \n",
    "            output = model(images)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if progr.cnt_total_iter % report_step == 0:\n",
    "                with torch.no_grad():\n",
    "                    acc1, acc5 = topk_accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "                    metric_proc({\n",
    "                        'loss': loss, \n",
    "                        'acc1': acc1, \n",
    "                        'acc5': acc5, \n",
    "                        Marker.ITERATION: progr.cnt_total_iter,\n",
    "                        **{f\"lr_{i}\": lr for i, lr in enumerate(scheduler.get_last_lr())},\n",
    "                    })\n",
    "\n",
    "#             if batch_idx == 0 and tb_writer:\n",
    "#                 publish_images(tb_writer, images, progr.cnt_total_iter)\n",
    "            \n",
    "#         scheduler.step()\n",
    "\n",
    "            \n",
    "\n",
    "    def validate_epoch(self, data_loader, model, metric_proc):\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.trainer_io.secondary_progress_bar(data_loader):\n",
    "                batch = to_device(batch, self.device)\n",
    "                images, target = batch\n",
    "                output = model(images)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                acc1, acc5 = topk_accuracy(output, target, topk=(1, 5))\n",
    "                metric_proc(dict(loss=loss, acc1=acc1, acc5=acc5))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from net import resnet_from_pytorch_cifar \n",
    "# model = resnet_from_pytorch_cifar.ResNet18()\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from net import resnet \n",
    "# model = resnet.ResNet(depth=20, num_classes=len(ds_tr.classes), block_name='BasicBlock', inplanes=64)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.AdamW([\n",
    "#     {\n",
    "#         'name': 'main_model',\n",
    "#         'params': model.parameters(),\n",
    "#         'lr': 1e-9,\n",
    "#         'weight_decay': 5e-4,\n",
    "#     }\n",
    "# ])\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-9,\n",
    "#                       momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_lr_finder import LRFinder, TrainDataLoaderIter\n",
    "\n",
    "# class LRFinderDL(TrainDataLoaderIter):\n",
    "#     def inputs_labels_from_batch(self, batch):\n",
    "#         return batch['image'], batch['target']\n",
    "\n",
    "# class LRFinderDL(TrainDataLoaderIter):\n",
    "#     def inputs_labels_from_batch(self, batch):\n",
    "#         return batch[0], batch[1]\n",
    "    \n",
    "    \n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "# lr_finder.range_test(LRFinderDL(train_dataloader), val_loader=None, end_lr=1, num_iter=100, step_mode=\"exp\")\n",
    "# _, recommended_lr = lr_finder.plot(log_lr=False)\n",
    "# lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='9' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      18.00% [9/50 05:08<23:25 {'avg.train.loss': 0.4098956286907196, 'avg.train.acc1': 0.8515625, 'avg.train.acc5': 0.998046875, 'avg.train.lr_0': 0.0984451603430247, 'avg.val.loss': 0.5979475899587704, 'avg.val.acc1': 0.8045886075949367, 'avg.val.acc5': 0.9892207278481012}]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='125' class='' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      31.97% [125/391 00:10<00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved meta data {'epoch': 0, 'model': 'epoch_00000.pth', 'scheduler': 'epoch_00000.scheduler.pth', 'optimizer': 'epoch_00000.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgdarr_onecpct0.1/checkpoints/epoch_00000.meta.json\n",
      "saved meta data {'epoch': 1, 'model': 'epoch_00001.pth', 'scheduler': 'epoch_00001.scheduler.pth', 'optimizer': 'epoch_00001.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgdarr_onecpct0.1/checkpoints/epoch_00001.meta.json\n",
      "saved meta data {'epoch': 2, 'model': 'epoch_00002.pth', 'scheduler': 'epoch_00002.scheduler.pth', 'optimizer': 'epoch_00002.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgdarr_onecpct0.1/checkpoints/epoch_00002.meta.json\n",
      "saved meta data {'epoch': 3, 'model': 'epoch_00003.pth', 'scheduler': 'epoch_00003.scheduler.pth', 'optimizer': 'epoch_00003.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgdarr_onecpct0.1/checkpoints/epoch_00003.meta.json\n",
      "saved meta data {'epoch': 4, 'model': 'epoch_00004.pth', 'scheduler': 'epoch_00004.scheduler.pth', 'optimizer': 'epoch_00004.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgdarr_onecpct0.1/checkpoints/epoch_00004.meta.json\n",
      "saved meta data {'epoch': 6, 'model': 'epoch_00006.pth', 'scheduler': 'epoch_00006.scheduler.pth', 'optimizer': 'epoch_00006.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgdarr_onecpct0.1/checkpoints/epoch_00006.meta.json\n",
      "saved meta data {'epoch': 8, 'model': 'epoch_00008.pth', 'scheduler': 'epoch_00008.scheduler.pth', 'optimizer': 'epoch_00008.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgdarr_onecpct0.1/checkpoints/epoch_00008.meta.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a4ac6ac52814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#     val_dataloader=list(islice(val_dataloader, 0, 5))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mval_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-10-920054762f04>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, optimizer, scheduler, start_epoch, end_epoch, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mmetric_proc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmod_name_train\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmetric_aggregator\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmetric_logger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mreport_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             )\n\u001b[1;32m     25\u001b[0m             self.validate_epoch(\n",
      "\u001b[0;32m<ipython-input-10-920054762f04>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, data_loader, progr, model, optimizer, scheduler, metric_proc, report_step)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p/lrn/lrnenv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p/lrn/lrnenv/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p/lrn/lrnenv/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                     \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mparam_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from net import resnet_from_pytorch_cifar \n",
    "model = resnet_from_pytorch_cifar.ResNet18()\n",
    "model\n",
    "\n",
    "num_epochs = 50\n",
    "optimizer = torch.optim.SGD([\n",
    "    {\n",
    "        'params': model.parameters(), \n",
    "        'lr': recommended_lr,\n",
    "        'momentum' :0.9, \n",
    "        'weight_decay': 5e-4\n",
    "    }\n",
    "])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=recommended_lr,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    pct_start=0.1,\n",
    ")\n",
    "\n",
    "trainer_io = TrainerIO(\n",
    "    log_dir=\"./logs/\", experiment_name=f\"cifar10_resnet18pc_lr{recommended_lr}_sgdarr_onecpct0.1\", \n",
    "    checkpoint_condition=HistoryCondition(\n",
    "        'avg.val.acc1', \n",
    "        lambda hist: len(hist) == 1 or hist[-1] > max(hist[:-1])\n",
    "    )\n",
    ")\n",
    "\n",
    "trainer = Trainer(device=device, trainer_io=trainer_io, continue_training=False)\n",
    "\n",
    "trainer.fit(\n",
    "    model, optimizer, scheduler,\n",
    "    start_epoch=0, end_epoch=num_epochs,\n",
    "#     train_dataloader=list(islice(train_dataloader, 0, 5)), \n",
    "#     val_dataloader=list(islice(val_dataloader, 0, 5))\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !rm checkpoints/epoch_00001*\n",
    "# !rm checkpoints/epoch_00002*\n",
    "# !rm checkpoints/epoch_00003*\n",
    "# !rm logs/experiment1/checkpoints/epoch_00004*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_lightning.callbacks import LearningRateLogger\n",
    "import numpy as np\n",
    "\n",
    "from model_base import ModelBase, get_main_model\n",
    "import pytorch_nn_tools as pnt\n",
    "from pytorch_nn_tools.visual import ImgShow\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Callable, Union\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ish = ImgShow(ax=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_dataset(path):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "\n",
    "    train_dir = os.path.join(path, 'train')\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        train_dir,\n",
    "        transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def _val_dataset(path):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "    val_dir = os.path.join(path, 'val')\n",
    "    dataset = datasets.ImageFolder(val_dir, transforms.Compose(\n",
    "        [transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize, ]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer = SummaryWriter(\"logs/lg1\")\n",
    "for i in range(0, 1000):\n",
    "    writer.add_scalar('train_loss', 1/(i+0.1), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "\n",
    "batch_size_train = 2\n",
    "batch_size_val = 2\n",
    "# batch_size_train = 128\n",
    "# batch_size_val = 16\n",
    "\n",
    "\n",
    "data_path = \"data/imagewoof2-320-cut/\"\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=_train_dataset(data_path),\n",
    "        batch_size=batch_size_train,\n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    \n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=_val_dataset(data_path),\n",
    "        batch_size=batch_size_val,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(batch, device):\n",
    "    return batch.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class HistoryCondition:\n",
    "    def __init__(self, metric_name: str, history_condition: Callable, history=()):\n",
    "        self.metric_name = metric_name\n",
    "        self.history = list(history[:])\n",
    "        self.condition = history_condition\n",
    "        \n",
    "    def __call__(self, metrics: Dict):\n",
    "        self.history.append(metrics[self.metric_name])\n",
    "        result = self.condition(self.history[:])\n",
    "#         print(f'Condition {result} on {self.history}')\n",
    "        return result\n",
    "            \n",
    "            \n",
    "class CheckpointSaver:\n",
    "    def __init__(self, path: Union[Path, str]):\n",
    "        self.path = Path(path)\n",
    "        self.path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    \n",
    "    def save(self, model, optimizer, scheduler, epoch):\n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.pth\")\n",
    "        print(f\"saving model to {path}\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "        \n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.optimizer.pth\")\n",
    "        print(f\"saving optimizer state to {path}\")\n",
    "        torch.save(optimizer.state_dict(), path)\n",
    "        \n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.scheduler.pth\")\n",
    "        print(f\"saving scheduler state to {path}\")\n",
    "        torch.save(scheduler.state_dict(), path)\n",
    "        \n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.meta.json\")\n",
    "        print(f\"saving meta data to {path}\")\n",
    "        with path.open(\"w\") as f:\n",
    "            json.dump({'epoch': epoch}, f)\n",
    "            \n",
    "    def load(self, model, optimizer, scheduler, epoch):\n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.pth\")\n",
    "        print(f\"loading model from {path}\")\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(path)\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        \n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.optimizer.pth\")\n",
    "        if path.exists():\n",
    "            print(f\"loading optimizer state from {path}\")\n",
    "            optimizer_dict = torch.load(path)\n",
    "            optimizer.load_state_dict(optimizer_dict)\n",
    "        else:\n",
    "            print(\"optimizer state not found\")\n",
    "            \n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.scheduler.pth\")\n",
    "        if path.exists():\n",
    "            print(f\"loading scheduler state from {path}\")\n",
    "            scheduler_dict = torch.load(path)\n",
    "            scheduler.load_state_dict(scheduler_dict)\n",
    "        else:\n",
    "            print(\"scheduler state not found\")\n",
    "\n",
    "    def find_last(self, start_epoch, end_epoch):\n",
    "        for epoch in range(end_epoch, start_epoch-1, -1):\n",
    "            path = self.path.joinpath(f\"epoch_{epoch:05d}.meta.json\")\n",
    "            if path.exists():\n",
    "                return epoch\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': 2}\n"
     ]
    }
   ],
   "source": [
    "d={'a':'b', 'c': 2}\n",
    "d.pop('a', None)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metrics = dict\n",
    "\n",
    "# class MetricBuilder:\n",
    "#     def __init__(self, required=()):\n",
    "#         self._required = required\n",
    "#         self._metrics = defaultdict(list)\n",
    "        \n",
    "#     def add(self, prefix, data, preproc_fn=lambda x: x.detach().item()):\n",
    "#         for r in self._required:\n",
    "#             assert r in data, f\"Metric {r} is required, but not provided\"\n",
    "            \n",
    "#         for k, v in data.items():\n",
    "#             self._metrics[f\"{prefix}_{k}\"].append(preproc_fn(v))\n",
    "            \n",
    "#     def build(self) -> Metrics:\n",
    "#         return {\n",
    "#             k: sum(vs) / (len(vs) if vs else 1.)\n",
    "#             for k, vs in self._metrics.items()\n",
    "#         }\n",
    "\n",
    "    \n",
    "class MetricProcessor:\n",
    "    def __call__(self, data, iteration=None):\n",
    "        return data\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        processors = []\n",
    "        for x in [self, other]:\n",
    "            if isinstance(x, MetricPipeline):\n",
    "                processors.extend(x._processors)\n",
    "            else:\n",
    "                processors.append(x)\n",
    "        return MetricPipeline(*processors)\n",
    "\n",
    "class MetricPipeline(MetricProcessor):\n",
    "    def __init__(self, *processors):\n",
    "        self._processors = processors\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        for p in self._processors:\n",
    "            data = p(data)\n",
    "        return data\n",
    "    \n",
    "    \n",
    "class MetricAggregator(MetricProcessor):\n",
    "    DEFAULT_SKIPPED = ('_iteration', '_epoch',)\n",
    "    def __init__(self, skipped=DEFAULT_SKIPPED):\n",
    "        self._metrics = defaultdict(list)\n",
    "        self.skipped = set(skipped)\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        for k, v in data.items():\n",
    "            if k not in self.skipped:\n",
    "                self._metrics[k].append(v)\n",
    "        return data\n",
    "            \n",
    "    def aggregate(self) -> Metrics:\n",
    "        return {\n",
    "            f\"avg.{k}\": sum(vs) / (len(vs) if vs else 1.)\n",
    "            for k, vs in self._metrics.items()\n",
    "        }\n",
    "    \n",
    "class MetricMod(MetricProcessor):\n",
    "    DEFAULT_SKIPPED = ('_iteration', '_epoch',)\n",
    "    def __init__(self, name_fn=lambda x: x, value_fn=lambda x: x.detach().item(), skipped=DEFAULT_SKIPPED):\n",
    "        self.name_fn = name_fn\n",
    "        self.value_fn = value_fn\n",
    "        self.skipped = set(skipped)\n",
    "        \n",
    "        \n",
    "    def __call__(self, data):\n",
    "        return dict([\n",
    "            (\n",
    "                (self.name_fn(name), self.value_fn(value)) \n",
    "                if name not in self.skipped \n",
    "                else (name, value)\n",
    "            )\n",
    "            for name, value in data.items()\n",
    "        ])\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "class MetricLogger(MetricProcessor):\n",
    "    def __init__(self, path):\n",
    "        path = Path(path)\n",
    "        path.mkdir(exist_ok=True, parents=True)\n",
    "        self.writer = SummaryWriter(path)\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        iteration = data.pop('_iteration', None)\n",
    "        if iteration is None:\n",
    "            iteration = data.pop('_epoch', None)\n",
    "        if iteration is not None:\n",
    "            for name, value in data.items():\n",
    "                self.writer.add_scalar(name, value, iteration)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train.asdas': 123, '_iteration': 456}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_name_train = MetricMod(\n",
    "    name_fn=lambda name: f\"train.{name}\",\n",
    ")\n",
    "mod_name_val = MetricMod(\n",
    "    name_fn=lambda name: f\"val.{name}\",\n",
    "    \n",
    ")\n",
    "\n",
    "mod_name_train({'asdas': torch.tensor(123), '_iteration': 456})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "class ProgressTracker:\n",
    "    def __init__(self, name=None):\n",
    "        self.cnt_total_iter = 0\n",
    "        self.name = name\n",
    "        \n",
    "    def track(self, dl):\n",
    "        def tracked_iterator():\n",
    "            for x in dl:\n",
    "                self.cnt_total_iter += 1\n",
    "                yield x\n",
    "        return tracked_iterator()\n",
    "\n",
    "# class ProgressTrackingDL:\n",
    "#     def __init__(self, dl, progress_tracker: ProgressTracker, parent_pbar=None):\n",
    "#         self.it = iter(progress_bar(dl, comment=progress_tracker.name, parent=parent_pbar))\n",
    "#         self.progress_tracker = progress_tracker\n",
    "        \n",
    "#     def __iter__(self):\n",
    "#         return self\n",
    "    \n",
    "#     def __next__(self):\n",
    "#         result = next(self.it)\n",
    "#         self.progress_tracker.cnt_total_iter += 1\n",
    "#         return result\n",
    "    \n",
    "#     @property\n",
    "#     def cnt_total_iter(self):\n",
    "#         return self.progress_tracker.cnt_total_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now_as_str():\n",
    "    now = datetime.datetime.now()\n",
    "    return now.strftime(\"%Y%m%d_%H%M%s_%f\")\n",
    "\n",
    "class PBars:\n",
    "    def __init__(self):\n",
    "        self._main = None\n",
    "        self._second = None\n",
    "        \n",
    "    def main(self, it, **kwargs):\n",
    "        self._main = master_bar(it, **kwargs)\n",
    "        return self._main\n",
    "    \n",
    "    def secondary(self, it, **kwargs):\n",
    "        if self._main is None:\n",
    "            raise RuntimeError(\"Cannot instantiate secondary progress bar. The main progress bar is not set.\")\n",
    "        self._second = progress_bar(it, parent=self._main, **kwargs)\n",
    "        return self._second\n",
    "        \n",
    "    def main_comment(self, comment):\n",
    "        self._main.main_bar.comment = comment\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, device, checkpoint_saver, checkpoint_condition,\n",
    "                continue_training: bool = False,\n",
    "                log_dir=\"./logs\",\n",
    "                name=\"model\"):\n",
    "        self.device = device\n",
    "        self.checkpoint_condition = checkpoint_condition\n",
    "        self.checkpoint_saver = checkpoint_saver\n",
    "        self.continue_training = continue_training\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.name = name\n",
    "        self.pbars = PBars()\n",
    "        \n",
    "        \n",
    "    def fit(self, model, optimizer, scheduler, start_epoch, end_epoch):\n",
    "        path_logs = Path(self.log_dir).joinpath(f\"{self.name}_{now_as_str()}\")\n",
    "        metric_logger = MetricLogger(path_logs)\n",
    "        model = model.to(self.device)\n",
    "        \n",
    "        if self.continue_training:\n",
    "            last = self.checkpoint_saver.find_last(start_epoch, end_epoch)\n",
    "            if last is not None:\n",
    "                print(f\"found pretrained results for epoch {last}. Loading...\")\n",
    "                self.checkpoint_saver.load(model, optimizer, scheduler, last)\n",
    "                start_epoch = last + 1\n",
    "\n",
    "        progr_train = ProgressTracker(name=\"train\")\n",
    "        \n",
    "        for epoch in self.pbars.main(range(start_epoch, end_epoch)):\n",
    "            metric_aggregator = MetricAggregator()\n",
    "            self.train_epoch(\n",
    "                train_dataloader, progr_train,\n",
    "                model, optimizer, scheduler,  \n",
    "                metric_proc=mod_name_train+metric_aggregator+metric_logger,\n",
    "                pbars=self.pbars,\n",
    "                report_step=3\n",
    "            )\n",
    "            self.validate_epoch(\n",
    "                val_dataloader,\n",
    "                model,  \n",
    "                metric_proc=mod_name_val+metric_aggregator+metric_logger,\n",
    "                pbars=self.pbars,\n",
    "            )\n",
    "            aggregated = metric_aggregator.aggregate()\n",
    "            metric_logger({**aggregated, '_epoch': epoch})\n",
    "            self.pbars.main_comment(f\"{aggregated}\")\n",
    "            \n",
    "            scheduler.step()\n",
    "                        \n",
    "            if self.checkpoint_condition(aggregated): \n",
    "                self.checkpoint_saver.save(model, optimizer, scheduler, epoch)\n",
    "            \n",
    "            metric_logger.close()\n",
    "        return path_logs\n",
    "            \n",
    "    def train_epoch(self, data_loader, progr, model, optimizer, scheduler, metric_proc, pbars, report_step=1):\n",
    "        model.train()\n",
    "                \n",
    "        for images, target in progr.track(pbars.secondary(data_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            images = to_device(images, self.device)\n",
    "            target = to_device(target, self.device)\n",
    "            \n",
    "            output = model(images)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if progr.cnt_total_iter % report_step == 0:\n",
    "                with torch.no_grad():\n",
    "                    acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "                metric_proc(dict(loss=loss, acc1=acc1, acc5=acc5, _iteration=progr.cnt_total_iter))\n",
    "                    \n",
    "        \n",
    "        \n",
    "    def validate_epoch(self, data_loader, model, metric_proc, pbars):\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, target in pbars.secondary(data_loader):\n",
    "                images = to_device(images, self.device)\n",
    "                target = to_device(target, self.device)\n",
    "\n",
    "                output = model(images)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "                \n",
    "                metric_proc(dict(loss=loss, acc1=acc1, acc5=acc5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm checkpoints/epo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model to checkpoints/epoch_00000.pth\n",
      "saving optimizer state to checkpoints/epoch_00000.optimizer.pth\n",
      "saving scheduler state to checkpoints/epoch_00000.scheduler.pth\n",
      "saving meta data to checkpoints/epoch_00000.meta.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s/work/nn/.venv/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model to checkpoints/epoch_00013.pth\n",
      "saving optimizer state to checkpoints/epoch_00013.optimizer.pth\n",
      "saving scheduler state to checkpoints/epoch_00013.scheduler.pth\n",
      "saving meta data to checkpoints/epoch_00013.meta.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/model_20201102_09291604305770_207814')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)        \n",
    "        \n",
    "checkpoint_condition = HistoryCondition(\n",
    "    'avg.val.acc1', \n",
    "    lambda hist: len(hist) == 1 or hist[-1] > max(hist[:-1])\n",
    ")\n",
    "checkpoint_saver = CheckpointSaver(path=\"checkpoints\")\n",
    "trainer = Trainer(\n",
    "    device='cpu', checkpoint_saver=checkpoint_saver, checkpoint_condition=checkpoint_condition,\n",
    "    continue_training=True\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD([\n",
    "    {\n",
    "        'name': 'main_model',\n",
    "        'params': model.parameters(),\n",
    "        'lr': 0.1,\n",
    "        'momentum': 0.9,\n",
    "        'weight_decay': 1e-4,\n",
    "    }\n",
    "])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lambda epoch: 0.1 ** (epoch // 30)\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model, optimizer, scheduler,\n",
    "    start_epoch=0, end_epoch=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !rm checkpoints/epoch_00001*\n",
    "# !rm checkpoints/epoch_00002*\n",
    "# !rm checkpoints/epoch_00003*\n",
    "# # !rm checkpoints/epoch_00004*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = models.resnet18(pretrained=True)        \n",
    "        \n",
    "# checkpoint_condition = HistoryCondition(\n",
    "#     'acc1', \n",
    "#     lambda hist: len(hist) == 1 or hist[-1] > max(hist[:-1])\n",
    "# )\n",
    "# checkpoint_saver = CheckpointSaver(path=\"checkpoints\")\n",
    "# trainer = Trainer(\n",
    "#     device='cuda', checkpoint_saver=checkpoint_saver, checkpoint_condition=checkpoint_condition,\n",
    "#     continue_training=True\n",
    "# )\n",
    "\n",
    "# optimizer = torch.optim.SGD([\n",
    "#     {\n",
    "#         'name': 'main_model',\n",
    "#         'params': model.parameters(),\n",
    "#         'lr': 0.1,\n",
    "#         'momentum': 0.9,\n",
    "#         'weight_decay': 1e-4,\n",
    "#     }\n",
    "# ])\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "#     optimizer,\n",
    "#     lambda epoch: 0.1 ** (epoch // 30)\n",
    "# )\n",
    "\n",
    "# trainer.fit(\n",
    "#     model, optimizer, scheduler,\n",
    "#     start_epoch=0, end_epoch=80\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ish.show_image(batch[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

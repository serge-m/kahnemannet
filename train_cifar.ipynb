{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/serge-m/pytorch-nn-tools.git@v0.3.3\n",
    "# !pip install torch_lr_finder==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U albumentations==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-nn-tools==0.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_lightning.callbacks import LearningRateLogger\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from model_base import ModelBase, get_main_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Callable, Union\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "\n",
    "import pytorch_nn_tools as pnt\n",
    "from pytorch_nn_tools.visual import ImgShow, tfm_vis_img, UnNormalize_, imagenet_stats\n",
    "from pytorch_nn_tools.train.metrics.processor import mod_name_train, mod_name_val, Marker\n",
    "from pytorch_nn_tools.train.metrics.processor import MetricAggregator, MetricLogger, MetricType\n",
    "from pytorch_nn_tools.train.progress import ProgressTracker\n",
    "from pytorch_nn_tools.convert import map_dict\n",
    "from pytorch_nn_tools.train.metrics.history_condition import HistoryCondition\n",
    "from pytorch_nn_tools.train.checkpoint import CheckpointSaver\n",
    "from pytorch_nn_tools.devices import to_device\n",
    "import ml_dataset_tools as mdt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??ProgressTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ish = ImgShow(ax=plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "\n",
    "batch_size_train, batch_size_val, device = 2, 2, 'cpu'\n",
    "batch_size_train, batch_size_val, device = 128, 128, 'cuda'\n",
    "\n",
    "data_root_path = Path(\"data/\")\n",
    "data_path = data_root_path.joinpath(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "size_h_w = 224, 224\n",
    "\n",
    "imagenet_stats = dict(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "cifar_stats = dict(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**cifar_stats),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**cifar_stats),\n",
    "])\n",
    "\n",
    "\n",
    "ds_tr  = datasets.CIFAR10(root=data_root_path, train=True, download=True, transform=transform_train)\n",
    "ds_val = datasets.CIFAR10(root=data_root_path, train=False, download=False, transform=transform_test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=ds_tr,\n",
    "        batch_size=batch_size_train,\n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    \n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=ds_val,\n",
    "        batch_size=batch_size_val,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBars:\n",
    "    def __init__(self):\n",
    "        self._main = None\n",
    "        self._second = None\n",
    "        \n",
    "    def main(self, it, **kwargs):\n",
    "        self._main = master_bar(it, **kwargs)\n",
    "        return self._main\n",
    "    \n",
    "    def secondary(self, it, **kwargs):\n",
    "        if self._main is None:\n",
    "            raise RuntimeError(\"Cannot instantiate secondary progress bar. The main progress bar is not set.\")\n",
    "        self._second = progress_bar(it, parent=self._main, **kwargs)\n",
    "        return self._second\n",
    "        \n",
    "    def main_comment(self, comment):\n",
    "        self._main.main_bar.comment = comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now_as_str():\n",
    "    now = datetime.datetime.now()\n",
    "    return now.strftime(\"%Y%m%d_%H%M%s_%f\")\n",
    "\n",
    "class DummyLogger:\n",
    "    def debug(self, *args):\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "class TrainerIO:\n",
    "    def __init__(self, log_dir: Union[Path, str], experiment_name: str, checkpoint_condition: Callable[[MetricType], bool]):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.experiment_name = experiment_name\n",
    "        self.path_experiment = self.log_dir.joinpath(experiment_name)\n",
    "        self.path_checkpoints = self.path_experiment.joinpath(\"checkpoints\")\n",
    "        self.checkpoint_saver = CheckpointSaver(self.path_checkpoints, logger=DummyLogger())\n",
    "        self.checkpoint_condition = checkpoint_condition\n",
    "    \n",
    "    def create_metric_logger(self):\n",
    "        path_logs = self.path_experiment.joinpath(f\"{self.experiment_name}_{now_as_str()}\")\n",
    "        metric_logger = MetricLogger(path_logs)\n",
    "        return metric_logger\n",
    "    \n",
    "    def load_last(self, start_epoch: int, end_epoch: int, model, optimizer, scheduler) -> int:\n",
    "        last = self.checkpoint_saver.find_last(start_epoch, end_epoch)\n",
    "        if last is not None:\n",
    "            print(f\"found pretrained results for epoch {last}. Loading...\")\n",
    "            self.checkpoint_saver.load(model, optimizer, scheduler, last)\n",
    "            return last + 1\n",
    "        else:\n",
    "            return start_epoch\n",
    "    \n",
    "    def save_checkpoint(self, metrics: MetricType, model, optimizer, scheduler, epoch):\n",
    "        if self.checkpoint_condition(metrics): \n",
    "            self.checkpoint_saver.save(model, optimizer, scheduler, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_images(tb_writer, images, iteration_id):\n",
    "    with torch.no_grad():\n",
    "        vis = images.detach().clone()\n",
    "        for v in vis:\n",
    "            v[:] = UnNormalize_(**cifar_stats)(v)\n",
    "        grid = torchvision.utils.make_grid(vis)\n",
    "        tb_writer.add_image('images', grid, iteration_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, device, trainer_io: TrainerIO,\n",
    "                continue_training: bool = False):\n",
    "        self.device = device\n",
    "        self.continue_training = continue_training\n",
    "        self.trainer_io = trainer_io\n",
    "        self.pbars = PBars()\n",
    "        \n",
    "        \n",
    "    def fit(self, model, optimizer, scheduler, start_epoch, end_epoch):\n",
    "        metric_logger = self.trainer_io.create_metric_logger()\n",
    "        model = model.to(self.device)\n",
    "        \n",
    "        if self.continue_training:\n",
    "            start_epoch = self.trainer_io.load_last(start_epoch, end_epoch, model, optimizer, scheduler)\n",
    "\n",
    "        progr_train = ProgressTracker()\n",
    "        \n",
    "        for epoch in self.pbars.main(range(start_epoch, end_epoch)):\n",
    "            metric_aggregator = MetricAggregator()\n",
    "            self.train_epoch(\n",
    "                train_dataloader, progr_train,\n",
    "                model, optimizer, scheduler,  \n",
    "                metric_proc=mod_name_train+metric_aggregator+metric_logger,\n",
    "                pbars=self.pbars,\n",
    "                report_step=10,\n",
    "                tb_writer=metric_logger.writer\n",
    "            )\n",
    "            self.validate_epoch(\n",
    "                val_dataloader,\n",
    "                model,  \n",
    "                metric_proc=mod_name_val+metric_aggregator+metric_logger,\n",
    "                pbars=self.pbars,\n",
    "                \n",
    "            )\n",
    "            \n",
    "            aggregated = map_dict(metric_aggregator.aggregate(), key_fn=lambda key: f\"avg.{key}\")\n",
    "            metric_logger({\n",
    "                **aggregated, \n",
    "                **{f\"lr_{i}\": lr for i, lr in enumerate(scheduler.get_last_lr())},\n",
    "                Marker.EPOCH: epoch,\n",
    "            })\n",
    "            self.pbars.main_comment(f\"{aggregated}\")\n",
    "            \n",
    "            \n",
    "                        \n",
    "            self.trainer_io.save_checkpoint(aggregated, model, optimizer, scheduler, epoch)\n",
    "            \n",
    "        metric_logger.close()\n",
    "            \n",
    "    def train_epoch(self, data_loader, progr, model, optimizer, scheduler, metric_proc, pbars, report_step=1,\n",
    "                   tb_writer=None):\n",
    "        model.train()\n",
    "                \n",
    "        for batch_idx, batch in enumerate(progr.track(pbars.secondary(data_loader))):\n",
    "            batch = to_device(batch, self.device)\n",
    "            images, target = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "                        \n",
    "            output = model(images)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "            \n",
    "            if progr.cnt_total_iter % report_step == 0:\n",
    "                with torch.no_grad():\n",
    "                    acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "                    metric_proc({\n",
    "                        'loss': loss, \n",
    "                        'acc1': acc1, \n",
    "                        'acc5': acc5, \n",
    "                        Marker.ITERATION: progr.cnt_total_iter,\n",
    "                        **{f\"lr_{i}\": lr for i, lr in enumerate(scheduler.get_last_lr())},\n",
    "                    })\n",
    "\n",
    "#             if batch_idx == 0 and tb_writer:\n",
    "#                 publish_images(tb_writer, images, progr.cnt_total_iter)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "            \n",
    "\n",
    "    def validate_epoch(self, data_loader, model, metric_proc, pbars):\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in pbars.secondary(data_loader):\n",
    "                batch = to_device(batch, self.device)\n",
    "                images, target = batch\n",
    "                output = model(images)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "                metric_proc(dict(loss=loss, acc1=acc1, acc5=acc5))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=False)        \n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\n",
    "        'name': 'main_model',\n",
    "        'params': model.parameters(),\n",
    "        'lr': 1e-8,\n",
    "        'weight_decay': 1e-4,\n",
    "    }\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_lr_finder import LRFinder, TrainDataLoaderIter\n",
    "\n",
    "# class LRFinderDL(TrainDataLoaderIter):\n",
    "#     def inputs_labels_from_batch(self, batch):\n",
    "#         return batch['image'], batch['target']\n",
    "\n",
    "# class LRFinderDL(TrainDataLoaderIter):\n",
    "#     def inputs_labels_from_batch(self, batch):\n",
    "#         return batch[0], batch[1]\n",
    "    \n",
    "    \n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "# lr_finder.range_test(LRFinderDL(train_dataloader), val_loader=None, end_lr=10, num_iter=50, step_mode=\"exp\")\n",
    "# _, recommended_lr = lr_finder.plot(log_lr=False)\n",
    "# lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved meta data {'epoch': 0, 'model': 'epoch_00000.pth', 'scheduler': 'epoch_00000.scheduler.pth', 'optimizer': 'epoch_00000.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00000.meta.json\n",
      "saved meta data {'epoch': 1, 'model': 'epoch_00001.pth', 'scheduler': 'epoch_00001.scheduler.pth', 'optimizer': 'epoch_00001.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00001.meta.json\n",
      "saved meta data {'epoch': 2, 'model': 'epoch_00002.pth', 'scheduler': 'epoch_00002.scheduler.pth', 'optimizer': 'epoch_00002.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00002.meta.json\n",
      "saved meta data {'epoch': 3, 'model': 'epoch_00003.pth', 'scheduler': 'epoch_00003.scheduler.pth', 'optimizer': 'epoch_00003.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00003.meta.json\n",
      "saved meta data {'epoch': 4, 'model': 'epoch_00004.pth', 'scheduler': 'epoch_00004.scheduler.pth', 'optimizer': 'epoch_00004.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00004.meta.json\n",
      "saved meta data {'epoch': 5, 'model': 'epoch_00005.pth', 'scheduler': 'epoch_00005.scheduler.pth', 'optimizer': 'epoch_00005.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00005.meta.json\n",
      "saved meta data {'epoch': 6, 'model': 'epoch_00006.pth', 'scheduler': 'epoch_00006.scheduler.pth', 'optimizer': 'epoch_00006.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00006.meta.json\n",
      "saved meta data {'epoch': 7, 'model': 'epoch_00007.pth', 'scheduler': 'epoch_00007.scheduler.pth', 'optimizer': 'epoch_00007.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00007.meta.json\n",
      "saved meta data {'epoch': 8, 'model': 'epoch_00008.pth', 'scheduler': 'epoch_00008.scheduler.pth', 'optimizer': 'epoch_00008.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00008.meta.json\n",
      "saved meta data {'epoch': 14, 'model': 'epoch_00014.pth', 'scheduler': 'epoch_00014.scheduler.pth', 'optimizer': 'epoch_00014.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00014.meta.json\n",
      "saved meta data {'epoch': 27, 'model': 'epoch_00027.pth', 'scheduler': 'epoch_00027.scheduler.pth', 'optimizer': 'epoch_00027.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00027.meta.json\n",
      "saved meta data {'epoch': 29, 'model': 'epoch_00029.pth', 'scheduler': 'epoch_00029.scheduler.pth', 'optimizer': 'epoch_00029.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00029.meta.json\n",
      "saved meta data {'epoch': 38, 'model': 'epoch_00038.pth', 'scheduler': 'epoch_00038.scheduler.pth', 'optimizer': 'epoch_00038.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00038.meta.json\n",
      "saved meta data {'epoch': 54, 'model': 'epoch_00054.pth', 'scheduler': 'epoch_00054.scheduler.pth', 'optimizer': 'epoch_00054.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00054.meta.json\n",
      "saved meta data {'epoch': 64, 'model': 'epoch_00064.pth', 'scheduler': 'epoch_00064.scheduler.pth', 'optimizer': 'epoch_00064.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00064.meta.json\n",
      "saved meta data {'epoch': 76, 'model': 'epoch_00076.pth', 'scheduler': 'epoch_00076.scheduler.pth', 'optimizer': 'epoch_00076.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00076.meta.json\n",
      "saved meta data {'epoch': 80, 'model': 'epoch_00080.pth', 'scheduler': 'epoch_00080.scheduler.pth', 'optimizer': 'epoch_00080.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00080.meta.json\n",
      "saved meta data {'epoch': 86, 'model': 'epoch_00086.pth', 'scheduler': 'epoch_00086.scheduler.pth', 'optimizer': 'epoch_00086.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00086.meta.json\n",
      "saved meta data {'epoch': 92, 'model': 'epoch_00092.pth', 'scheduler': 'epoch_00092.scheduler.pth', 'optimizer': 'epoch_00092.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00092.meta.json\n",
      "saved meta data {'epoch': 99, 'model': 'epoch_00099.pth', 'scheduler': 'epoch_00099.scheduler.pth', 'optimizer': 'epoch_00099.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00099.meta.json\n",
      "saved meta data {'epoch': 102, 'model': 'epoch_00102.pth', 'scheduler': 'epoch_00102.scheduler.pth', 'optimizer': 'epoch_00102.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00102.meta.json\n",
      "saved meta data {'epoch': 105, 'model': 'epoch_00105.pth', 'scheduler': 'epoch_00105.scheduler.pth', 'optimizer': 'epoch_00105.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00105.meta.json\n",
      "saved meta data {'epoch': 121, 'model': 'epoch_00121.pth', 'scheduler': 'epoch_00121.scheduler.pth', 'optimizer': 'epoch_00121.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00121.meta.json\n",
      "saved meta data {'epoch': 122, 'model': 'epoch_00122.pth', 'scheduler': 'epoch_00122.scheduler.pth', 'optimizer': 'epoch_00122.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00122.meta.json\n",
      "saved meta data {'epoch': 127, 'model': 'epoch_00127.pth', 'scheduler': 'epoch_00127.scheduler.pth', 'optimizer': 'epoch_00127.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00127.meta.json\n",
      "saved meta data {'epoch': 135, 'model': 'epoch_00135.pth', 'scheduler': 'epoch_00135.scheduler.pth', 'optimizer': 'epoch_00135.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00135.meta.json\n",
      "saved meta data {'epoch': 137, 'model': 'epoch_00137.pth', 'scheduler': 'epoch_00137.scheduler.pth', 'optimizer': 'epoch_00137.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00137.meta.json\n",
      "saved meta data {'epoch': 138, 'model': 'epoch_00138.pth', 'scheduler': 'epoch_00138.scheduler.pth', 'optimizer': 'epoch_00138.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00138.meta.json\n",
      "saved meta data {'epoch': 140, 'model': 'epoch_00140.pth', 'scheduler': 'epoch_00140.scheduler.pth', 'optimizer': 'epoch_00140.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00140.meta.json\n",
      "saved meta data {'epoch': 141, 'model': 'epoch_00141.pth', 'scheduler': 'epoch_00141.scheduler.pth', 'optimizer': 'epoch_00141.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00141.meta.json\n",
      "saved meta data {'epoch': 142, 'model': 'epoch_00142.pth', 'scheduler': 'epoch_00142.scheduler.pth', 'optimizer': 'epoch_00142.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00142.meta.json\n",
      "saved meta data {'epoch': 152, 'model': 'epoch_00152.pth', 'scheduler': 'epoch_00152.scheduler.pth', 'optimizer': 'epoch_00152.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00152.meta.json\n",
      "saved meta data {'epoch': 155, 'model': 'epoch_00155.pth', 'scheduler': 'epoch_00155.scheduler.pth', 'optimizer': 'epoch_00155.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00155.meta.json\n",
      "saved meta data {'epoch': 156, 'model': 'epoch_00156.pth', 'scheduler': 'epoch_00156.scheduler.pth', 'optimizer': 'epoch_00156.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00156.meta.json\n",
      "saved meta data {'epoch': 158, 'model': 'epoch_00158.pth', 'scheduler': 'epoch_00158.scheduler.pth', 'optimizer': 'epoch_00158.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00158.meta.json\n",
      "saved meta data {'epoch': 159, 'model': 'epoch_00159.pth', 'scheduler': 'epoch_00159.scheduler.pth', 'optimizer': 'epoch_00159.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00159.meta.json\n",
      "saved meta data {'epoch': 162, 'model': 'epoch_00162.pth', 'scheduler': 'epoch_00162.scheduler.pth', 'optimizer': 'epoch_00162.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00162.meta.json\n",
      "saved meta data {'epoch': 163, 'model': 'epoch_00163.pth', 'scheduler': 'epoch_00163.scheduler.pth', 'optimizer': 'epoch_00163.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00163.meta.json\n",
      "saved meta data {'epoch': 167, 'model': 'epoch_00167.pth', 'scheduler': 'epoch_00167.scheduler.pth', 'optimizer': 'epoch_00167.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00167.meta.json\n",
      "saved meta data {'epoch': 170, 'model': 'epoch_00170.pth', 'scheduler': 'epoch_00170.scheduler.pth', 'optimizer': 'epoch_00170.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00170.meta.json\n",
      "saved meta data {'epoch': 173, 'model': 'epoch_00173.pth', 'scheduler': 'epoch_00173.scheduler.pth', 'optimizer': 'epoch_00173.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00173.meta.json\n",
      "saved meta data {'epoch': 175, 'model': 'epoch_00175.pth', 'scheduler': 'epoch_00175.scheduler.pth', 'optimizer': 'epoch_00175.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00175.meta.json\n",
      "saved meta data {'epoch': 176, 'model': 'epoch_00176.pth', 'scheduler': 'epoch_00176.scheduler.pth', 'optimizer': 'epoch_00176.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00176.meta.json\n",
      "saved meta data {'epoch': 178, 'model': 'epoch_00178.pth', 'scheduler': 'epoch_00178.scheduler.pth', 'optimizer': 'epoch_00178.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00178.meta.json\n",
      "saved meta data {'epoch': 179, 'model': 'epoch_00179.pth', 'scheduler': 'epoch_00179.scheduler.pth', 'optimizer': 'epoch_00179.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00179.meta.json\n",
      "saved meta data {'epoch': 182, 'model': 'epoch_00182.pth', 'scheduler': 'epoch_00182.scheduler.pth', 'optimizer': 'epoch_00182.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00182.meta.json\n",
      "saved meta data {'epoch': 185, 'model': 'epoch_00185.pth', 'scheduler': 'epoch_00185.scheduler.pth', 'optimizer': 'epoch_00185.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00185.meta.json\n",
      "saved meta data {'epoch': 188, 'model': 'epoch_00188.pth', 'scheduler': 'epoch_00188.scheduler.pth', 'optimizer': 'epoch_00188.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00188.meta.json\n",
      "saved meta data {'epoch': 199, 'model': 'epoch_00199.pth', 'scheduler': 'epoch_00199.scheduler.pth', 'optimizer': 'epoch_00199.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00199.meta.json\n",
      "saved meta data {'epoch': 200, 'model': 'epoch_00200.pth', 'scheduler': 'epoch_00200.scheduler.pth', 'optimizer': 'epoch_00200.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00200.meta.json\n",
      "saved meta data {'epoch': 207, 'model': 'epoch_00207.pth', 'scheduler': 'epoch_00207.scheduler.pth', 'optimizer': 'epoch_00207.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00207.meta.json\n",
      "saved meta data {'epoch': 210, 'model': 'epoch_00210.pth', 'scheduler': 'epoch_00210.scheduler.pth', 'optimizer': 'epoch_00210.optimizer.pth'} to logs/cifar10_resnet18_lr0.1_sgd/checkpoints/epoch_00210.meta.json\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=False)        \n",
    "\n",
    "# optimizer = torch.optim.AdamW([\n",
    "#     {\n",
    "#         'name': 'main_model',\n",
    "#         'params': model.parameters(),\n",
    "#         'lr': recommended_lr,\n",
    "#         'weight_decay': 1e-4,\n",
    "#     }\n",
    "# ])\n",
    "\n",
    "# num_epochs = 164\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#     optimizer,\n",
    "#     max_lr=recommended_lr,\n",
    "#     epochs=num_epochs,\n",
    "#     steps_per_epoch=len(train_dataloader)\n",
    "# )\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "#     optimizer, \n",
    "#     milestones=[81, 122], \n",
    "#     gamma=0.1, \n",
    "# )\n",
    "\n",
    "num_epochs = 500\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=recommended_lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "trainer_io = TrainerIO(\n",
    "    log_dir=\"./logs/\", experiment_name=f\"cifar10_resnet18_lr{recommended_lr}_sgd\", \n",
    "    checkpoint_condition=HistoryCondition(\n",
    "        'avg.val.acc1', \n",
    "        lambda hist: len(hist) == 1 or hist[-1] > max(hist[:-1])\n",
    "    )\n",
    ")\n",
    "\n",
    "trainer = Trainer(device=device, trainer_io=trainer_io, continue_training=False)\n",
    "\n",
    "trainer.fit(\n",
    "    model, optimizer, scheduler,\n",
    "    start_epoch=0, end_epoch=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !rm checkpoints/epoch_00001*\n",
    "# !rm checkpoints/epoch_00002*\n",
    "# !rm checkpoints/epoch_00003*\n",
    "# !rm logs/experiment1/checkpoints/epoch_00004*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

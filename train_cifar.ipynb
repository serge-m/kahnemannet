{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/serge-m/pytorch-nn-tools.git@v0.3.5\n",
    "# !pip install torch_lr_finder==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U albumentations==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-nn-tools==0.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_lightning.callbacks import LearningRateLogger\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from model_base import ModelBase, get_main_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Callable, Union\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "\n",
    "import pytorch_nn_tools as pnt\n",
    "from pytorch_nn_tools.visual import ImgShow, tfm_vis_img, UnNormalize_, imagenet_stats\n",
    "from pytorch_nn_tools.train.metrics.processor import mod_name_train, mod_name_val, Marker\n",
    "from pytorch_nn_tools.train.metrics.processor import MetricAggregator, MetricType, TensorBoardMetricLogger\n",
    "from pytorch_nn_tools.metrics.accuracy import topk_accuracy\n",
    "from pytorch_nn_tools.train.progress import ProgressTracker\n",
    "from pytorch_nn_tools.convert import map_dict\n",
    "from pytorch_nn_tools.train.metrics.history_condition import HistoryCondition\n",
    "from pytorch_nn_tools.train.checkpoint import CheckpointSaver\n",
    "from pytorch_nn_tools.devices import to_device\n",
    "import ml_dataset_tools as mdt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??ProgressTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ish = ImgShow(ax=plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "\n",
    "batch_size_train, batch_size_val, device = 2, 2, 'cpu'\n",
    "# batch_size_train, batch_size_val, device = 128, 128, 'cuda'\n",
    "\n",
    "data_root_path = Path(\"data/\")\n",
    "data_path = data_root_path.joinpath(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_h_w = 224, 224\n",
    "\n",
    "imagenet_stats = dict(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "cifar_stats = dict(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**cifar_stats),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**cifar_stats),\n",
    "])\n",
    "\n",
    "\n",
    "ds_tr  = datasets.CIFAR10(root=data_root_path, train=True, download=True, transform=transform_train)\n",
    "ds_val = datasets.CIFAR10(root=data_root_path, train=False, download=False, transform=transform_test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=ds_tr,\n",
    "        batch_size=batch_size_train,\n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    \n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=ds_val,\n",
    "        batch_size=batch_size_val,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBars:\n",
    "    def __init__(self):\n",
    "        self._main = None\n",
    "        self._second = None\n",
    "        \n",
    "    def main(self, it, **kwargs):\n",
    "        self._main = master_bar(it, **kwargs)\n",
    "        return self._main\n",
    "    \n",
    "    def secondary(self, it, **kwargs):\n",
    "        if self._main is None:\n",
    "            raise RuntimeError(\"Cannot instantiate secondary progress bar. The main progress bar is not set.\")\n",
    "        self._second = progress_bar(it, parent=self._main, **kwargs)\n",
    "        return self._second\n",
    "        \n",
    "    def main_comment(self, comment):\n",
    "        self._main.main_bar.comment = comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def now_as_str():\n",
    "    now = datetime.datetime.now()\n",
    "    return now.strftime(\"%Y%m%d_%H%M%s_%f\")\n",
    "\n",
    "class DummyLogger:\n",
    "    def debug(self, *args):\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "class TrainerIO:\n",
    "    def __init__(self, log_dir: Union[Path, str], experiment_name: str, checkpoint_condition: Callable[[MetricType], bool]):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.experiment_name = experiment_name\n",
    "        self.path_experiment = self.log_dir.joinpath(experiment_name)\n",
    "        self.path_checkpoints = self.path_experiment.joinpath(\"checkpoints\")\n",
    "        \n",
    "        self.checkpoint_saver = CheckpointSaver(self.path_checkpoints, logger=DummyLogger())\n",
    "        self.checkpoint_condition = checkpoint_condition\n",
    "        \n",
    "        path_logs = self.path_experiment.joinpath(f\"{self.experiment_name}_{now_as_str()}\")\n",
    "        path_logs.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        self.tb_summary_writer = SummaryWriter(path_logs)\n",
    "        \n",
    "        self.pbars = PBars()\n",
    "    \n",
    "    def load_last(self, start_epoch: int, end_epoch: int, model, optimizer, scheduler) -> int:\n",
    "        last = self.checkpoint_saver.find_last(start_epoch, end_epoch)\n",
    "        if last is not None:\n",
    "            print(f\"found pretrained results for epoch {last}. Loading...\")\n",
    "            self.checkpoint_saver.load(model, optimizer, scheduler, last)\n",
    "            return last + 1\n",
    "        else:\n",
    "            return start_epoch\n",
    "    \n",
    "    def save_checkpoint(self, metrics: MetricType, model, optimizer, scheduler, epoch):\n",
    "        if self.checkpoint_condition(metrics): \n",
    "            self.checkpoint_saver.save(model, optimizer, scheduler, epoch)\n",
    "            \n",
    "    def main_progress_bar(self, iterable):\n",
    "        return self.pbars.main(iterable)\n",
    "    \n",
    "    def secondary_progress_bar(self, iterable):\n",
    "        return self.pbars.secondary(iterable)\n",
    "    \n",
    "    def set_main_status_msg(self, value):\n",
    "        self.pbars.main_comment(value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_images(tb_writer, images, iteration_id):\n",
    "    with torch.no_grad():\n",
    "        vis = images.detach().clone()\n",
    "        for v in vis:\n",
    "            v[:] = UnNormalize_(**cifar_stats)(v)\n",
    "        grid = torchvision.utils.make_grid(vis)\n",
    "        tb_writer.add_image('images', grid, iteration_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, device, trainer_io: TrainerIO,\n",
    "                continue_training: bool = False):\n",
    "        self.device = device\n",
    "        self.continue_training = continue_training\n",
    "        self.trainer_io = trainer_io\n",
    "        \n",
    "    def fit(self, model, optimizer, scheduler, start_epoch, end_epoch, train_dataloader, val_dataloader):\n",
    "        metric_logger = TensorBoardMetricLogger(self.trainer_io.tb_summary_writer)\n",
    "        model = to_device(model, self.device)\n",
    "        \n",
    "        if self.continue_training:\n",
    "            start_epoch = self.trainer_io.load_last(start_epoch, end_epoch, model, optimizer, scheduler)\n",
    "\n",
    "        progr_train = ProgressTracker()\n",
    "        \n",
    "        for epoch in self.trainer_io.main_progress_bar(range(start_epoch, end_epoch)):\n",
    "            metric_aggregator = MetricAggregator()\n",
    "            self.train_epoch(\n",
    "                train_dataloader, progr_train,\n",
    "                model, optimizer, scheduler,  \n",
    "                metric_proc=mod_name_train+metric_aggregator+metric_logger,\n",
    "                report_step=100,\n",
    "            )\n",
    "            self.validate_epoch(\n",
    "                val_dataloader,\n",
    "                model,  \n",
    "                metric_proc=mod_name_val+metric_aggregator+metric_logger,\n",
    "            )\n",
    "            \n",
    "            aggregated = map_dict(metric_aggregator.aggregate(), key_fn=lambda key: f\"avg.{key}\")\n",
    "            metric_logger({\n",
    "                **aggregated, \n",
    "                **{f\"lr_{i}\": lr for i, lr in enumerate(scheduler.get_last_lr())},\n",
    "                Marker.EPOCH: epoch,\n",
    "            })\n",
    "            self.trainer_io.set_main_status_msg(f\"{aggregated}\")\n",
    "            self.trainer_io.save_checkpoint(aggregated, model, optimizer, scheduler, epoch)\n",
    "            \n",
    "        metric_logger.close()\n",
    "            \n",
    "    def train_epoch(self, data_loader, progr, model, optimizer, scheduler, metric_proc, report_step):\n",
    "        model.train()\n",
    "                \n",
    "        for batch in self.trainer_io.secondary_progress_bar(progr.track(data_loader)):\n",
    "            batch = to_device(batch, self.device)\n",
    "            images, target = batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "                        \n",
    "            output = model(images)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "            \n",
    "            if progr.cnt_total_iter % report_step == 0 or 1:\n",
    "                with torch.no_grad():\n",
    "                    acc1, acc5 = topk_accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "                    metric_proc({\n",
    "                        'loss': loss, \n",
    "                        'acc1': acc1, \n",
    "                        'acc5': acc5, \n",
    "                        Marker.ITERATION: progr.cnt_total_iter,\n",
    "                        **{f\"lr_{i}\": lr for i, lr in enumerate(scheduler.get_last_lr())},\n",
    "                    })\n",
    "\n",
    "#             if batch_idx == 0 and tb_writer:\n",
    "#                 publish_images(tb_writer, images, progr.cnt_total_iter)\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "            \n",
    "\n",
    "    def validate_epoch(self, data_loader, model, metric_proc):\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.trainer_io.secondary_progress_bar(data_loader):\n",
    "                batch = to_device(batch, self.device)\n",
    "                images, target = batch\n",
    "                output = model(images)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                acc1, acc5 = topk_accuracy(output, target, topk=(1, 5))\n",
    "                metric_proc(dict(loss=loss, acc1=acc1, acc5=acc5))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet18(pretrained=False)        \n",
    "\n",
    "# optimizer = torch.optim.AdamW([\n",
    "#     {\n",
    "#         'name': 'main_model',\n",
    "#         'params': model.parameters(),\n",
    "#         'lr': 1e-8,\n",
    "#         'weight_decay': 1e-4,\n",
    "#     }\n",
    "# ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from net import resnet_from_pytorch_cifar \n",
    "# model = resnet_from_pytorch_cifar.ResNet18()\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds_tr.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import resnet \n",
    "model = resnet.ResNet(depth=20, num_classes=len(ds_tr.classes), block_name='BasicBlock', inplanes=64)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_lr_finder import LRFinder, TrainDataLoaderIter\n",
    "\n",
    "# class LRFinderDL(TrainDataLoaderIter):\n",
    "#     def inputs_labels_from_batch(self, batch):\n",
    "#         return batch['image'], batch['target']\n",
    "\n",
    "# class LRFinderDL(TrainDataLoaderIter):\n",
    "#     def inputs_labels_from_batch(self, batch):\n",
    "#         return batch[0], batch[1]\n",
    "    \n",
    "    \n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "# lr_finder.range_test(LRFinderDL(train_dataloader), val_loader=None, end_lr=10, num_iter=50, step_mode=\"exp\")\n",
    "# _, recommended_lr = lr_finder.plot(log_lr=False)\n",
    "# lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimizer = torch.optim.AdamW([\n",
    "#     {\n",
    "#         'name': 'main_model',\n",
    "#         'params': model.parameters(),\n",
    "#         'lr': recommended_lr,\n",
    "#         'weight_decay': 1e-4,\n",
    "#     }\n",
    "# ])\n",
    "\n",
    "# num_epochs = 164\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#     optimizer,\n",
    "#     max_lr=recommended_lr,\n",
    "#     epochs=num_epochs,\n",
    "#     steps_per_epoch=len(train_dataloader)\n",
    "# )\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "#     optimizer, \n",
    "#     milestones=[81, 122], \n",
    "#     gamma=0.1, \n",
    "# )\n",
    "\n",
    "num_epochs = 200\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=recommended_lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "trainer_io = TrainerIO(\n",
    "    log_dir=\"./logs/\", experiment_name=f\"cifar10_resnet20w64_lr{recommended_lr}_sgd\", \n",
    "    checkpoint_condition=HistoryCondition(\n",
    "        'avg.val.acc1', \n",
    "        lambda hist: len(hist) == 1 or hist[-1] > max(hist[:-1])\n",
    "    )\n",
    ")\n",
    "\n",
    "trainer = Trainer(device=device, trainer_io=trainer_io, continue_training=False)\n",
    "\n",
    "trainer.fit(\n",
    "    model, optimizer, scheduler,\n",
    "    start_epoch=0, end_epoch=num_epochs,\n",
    "    train_dataloader=list(islice(train_dataloader, 0, 5)), \n",
    "    val_dataloader=list(islice(val_dataloader, 0, 5))\n",
    "#     train_dataloader=train_dataloader,\n",
    "#     val_dataloader=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !rm checkpoints/epoch_00001*\n",
    "# !rm checkpoints/epoch_00002*\n",
    "# !rm checkpoints/epoch_00003*\n",
    "# !rm logs/experiment1/checkpoints/epoch_00004*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

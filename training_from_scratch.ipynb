{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_lightning.callbacks import LearningRateLogger\n",
    "import numpy as np\n",
    "\n",
    "from model_base import ModelBase, get_main_model\n",
    "import pytorch_nn_tools as pnt\n",
    "from pytorch_nn_tools.visual import ImgShow\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Callable, Union\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ish = ImgShow(ax=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_dataset(path):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "\n",
    "    train_dir = os.path.join(path, 'train')\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        train_dir,\n",
    "        transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def _val_dataset(path):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "    val_dir = os.path.join(path, 'val')\n",
    "    dataset = datasets.ImageFolder(val_dir, transforms.Compose(\n",
    "        [transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize, ]))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "\n",
    "# batch_size_train = 2\n",
    "# batch_size_val = 2\n",
    "batch_size_train = 128\n",
    "batch_size_val = 16\n",
    "\n",
    "\n",
    "data_path = \"data/imagewoof2-320/\"\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=_train_dataset(data_path),\n",
    "        batch_size=batch_size_train,\n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    \n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=_val_dataset(data_path),\n",
    "        batch_size=batch_size_val,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(batch, device):\n",
    "    return batch.to(device)\n",
    "\n",
    "Metrics = dict\n",
    "\n",
    "class MetricBuilder:\n",
    "    def __init__(self, required=()):\n",
    "        self._required = required\n",
    "        self._metrics = defaultdict(list)\n",
    "        \n",
    "    def add(self, metrics_dict, preproc_fn=lambda x: x.detach().item()):\n",
    "        for r in self._required:\n",
    "            assert r in metrics_dict, f\"Metric {r} is required, but not provided\"\n",
    "            \n",
    "        for k, v in metrics_dict.items():\n",
    "            self._metrics[k].append(preproc_fn(v))\n",
    "            \n",
    "    def build(self) -> Metrics:\n",
    "        return {\n",
    "            k: sum(vs) / (len(vs) if vs else 1.)\n",
    "            for k, vs in self._metrics.items()\n",
    "        }\n",
    "    \n",
    "\n",
    "    \n",
    "class HistoryCondition:\n",
    "    def __init__(self, metric_name: Metrics, history_condition: Callable, history=()):\n",
    "        self.metric_name = metric_name\n",
    "        self.history = list(history[:])\n",
    "        self.condition = history_condition\n",
    "        \n",
    "    def __call__(self, metrics: Metrics):\n",
    "        self.history.append(metrics[self.metric_name])\n",
    "        result = self.condition(self.history[:])\n",
    "#         print(f'Condition {result} on {self.history}')\n",
    "        return result\n",
    "            \n",
    "            \n",
    "class CheckpointSaver:\n",
    "    def __init__(self, path: Union[Path, str]):\n",
    "        self.path = Path(path)\n",
    "        self.path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    \n",
    "    def save(self, model, optimizer, scheduler, epoch):\n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.pth\")\n",
    "        print(f\"saving model to {path}\")\n",
    "        torch.save(model.state_dict(), path)\n",
    "        \n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.optimizer.pth\")\n",
    "        print(f\"saving optimizer state to {path}\")\n",
    "        torch.save(optimizer.state_dict(), path)\n",
    "        \n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.scheduler.pth\")\n",
    "        print(f\"saving scheduler state to {path}\")\n",
    "        torch.save(scheduler.state_dict(), path)\n",
    "        \n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.meta.json\")\n",
    "        print(f\"saving meta data to {path}\")\n",
    "        with path.open(\"w\") as f:\n",
    "            json.dump({'epoch': epoch}, f)\n",
    "            \n",
    "    def load(self, model, optimizer, scheduler, epoch):\n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.pth\")\n",
    "        print(f\"loading model from {path}\")\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(path)\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        \n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.optimizer.pth\")\n",
    "        if path.exists():\n",
    "            print(f\"loading optimizer state from {path}\")\n",
    "            optimizer_dict = torch.load(path)\n",
    "            optimizer.load_state_dict(optimizer_dict)\n",
    "        else:\n",
    "            print(\"optimizer state not found\")\n",
    "            \n",
    "        path = self.path.joinpath(f\"epoch_{epoch:05d}.scheduler.pth\")\n",
    "        if path.exists():\n",
    "            print(f\"loading scheduler state from {path}\")\n",
    "            scheduler_dict = torch.load(path)\n",
    "            scheduler.load_state_dict(scheduler_dict)\n",
    "        else:\n",
    "            print(\"scheduler state not found\")\n",
    "\n",
    "    def find_last(self, start_epoch, end_epoch):\n",
    "        for epoch in range(end_epoch, start_epoch-1, -1):\n",
    "            path = self.path.joinpath(f\"epoch_{epoch:05d}.meta.json\")\n",
    "            if path.exists():\n",
    "                return epoch\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class Trainer:\n",
    "    def __init__(self, device, checkpoint_saver, checkpoint_condition,\n",
    "                continue_training: bool = False):\n",
    "        self.device = device\n",
    "        self.checkpoint_condition = checkpoint_condition\n",
    "        self.checkpoint_saver = checkpoint_saver\n",
    "        self.continue_training = continue_training\n",
    "        \n",
    "    def fit(self, model, optimizer, scheduler, start_epoch, end_epoch):\n",
    "        model = model.to(self.device)\n",
    "        if self.continue_training:\n",
    "            last = self.checkpoint_saver.find_last(start_epoch, end_epoch)\n",
    "            if last is not None:\n",
    "                print(f\"found pretrained results for epoch {last}. Loading...\")\n",
    "                self.checkpoint_saver.load(model, optimizer, scheduler, last)\n",
    "                start_epoch = last + 1\n",
    "\n",
    "                \n",
    "        for epoch in range(start_epoch, end_epoch):\n",
    "            metrics_train = self.train_epoch(model, optimizer, scheduler)\n",
    "            self.log(metrics_train)\n",
    "            \n",
    "            metrics_val = self.validate_epoch(model)\n",
    "            self.log(metrics_val)\n",
    "            \n",
    "            scheduler.step()\n",
    "                        \n",
    "            if self.checkpoint_condition(metrics_val): \n",
    "                self.checkpoint_saver.save(model, optimizer, scheduler, epoch)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def train_epoch(self, model, optimizer, scheduler):\n",
    "        model.train()\n",
    "        metrics = MetricBuilder()\n",
    "        \n",
    "        for images, target in self._progress_bar(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            images = to_device(images, self.device)\n",
    "            target = to_device(target, self.device)\n",
    "            \n",
    "            output = model(images)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            metrics.add(dict(loss=loss, acc1=acc1, acc5=acc5))\n",
    "            \n",
    "        return metrics.build()\n",
    "        \n",
    "        \n",
    "    def validate_epoch(self, model):\n",
    "        model.eval()\n",
    "        metrics = MetricBuilder()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, target in self._progress_bar(val_dataloader):\n",
    "                images = to_device(images, self.device)\n",
    "                target = to_device(target, self.device)\n",
    "\n",
    "                output = model(images)\n",
    "                loss = F.cross_entropy(output, target)\n",
    "                acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "                metrics.add(dict(loss=loss, acc1=acc1, acc5=acc5))\n",
    "                \n",
    "        return metrics.build()\n",
    "        \n",
    "    \n",
    "    def _progress_bar(self, it):\n",
    "        return tqdm(it)\n",
    "    \n",
    "    def log(self, data):\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'checkpoints/epo*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm checkpoints/epo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 9/71 [00:03<00:21,  2.90it/s]"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)        \n",
    "        \n",
    "checkpoint_condition = HistoryCondition(\n",
    "    'acc1', \n",
    "    lambda hist: len(hist) == 1 or hist[-1] > max(hist[:-1])\n",
    ")\n",
    "checkpoint_saver = CheckpointSaver(path=\"checkpoints\")\n",
    "trainer = Trainer(\n",
    "    device='cuda', checkpoint_saver=checkpoint_saver, checkpoint_condition=checkpoint_condition,\n",
    "    continue_training=True\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD([\n",
    "    {\n",
    "        'name': 'main_model',\n",
    "        'params': model.parameters(),\n",
    "        'lr': 0.1,\n",
    "        'momentum': 0.9,\n",
    "        'weight_decay': 1e-4,\n",
    "    }\n",
    "])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lambda epoch: 0.1 ** (epoch // 30)\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model, optimizer, scheduler,\n",
    "    start_epoch=0, end_epoch=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !rm checkpoints/epoch_00001*\n",
    "# !rm checkpoints/epoch_00002*\n",
    "# !rm checkpoints/epoch_00003*\n",
    "# # !rm checkpoints/epoch_00004*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)        \n",
    "        \n",
    "checkpoint_condition = HistoryCondition(\n",
    "    'acc1', \n",
    "    lambda hist: len(hist) == 1 or hist[-1] > max(hist[:-1])\n",
    ")\n",
    "checkpoint_saver = CheckpointSaver(path=\"checkpoints\")\n",
    "trainer = Trainer(\n",
    "    device='cuda', checkpoint_saver=checkpoint_saver, checkpoint_condition=checkpoint_condition,\n",
    "    continue_training=True\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD([\n",
    "    {\n",
    "        'name': 'main_model',\n",
    "        'params': model.parameters(),\n",
    "        'lr': 0.1,\n",
    "        'momentum': 0.9,\n",
    "        'weight_decay': 1e-4,\n",
    "    }\n",
    "])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lambda epoch: 0.1 ** (epoch // 30)\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model, optimizer, scheduler,\n",
    "    start_epoch=0, end_epoch=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet18(pretrained=True)        \n",
    "        \n",
    "# trainer = Trainer(device='cuda')\n",
    "\n",
    "# optimizer = torch.optim.SGD([\n",
    "#     {\n",
    "#         'name': 'main_model',\n",
    "#         'params': model.parameters(),\n",
    "#         'lr': 0.1,\n",
    "#         'momentum': 0.9,\n",
    "#         'weight_decay': 1e-4,\n",
    "#     }\n",
    "# ])\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "#     optimizer,\n",
    "#     lambda epoch: 0.1 ** (epoch // 30)\n",
    "# )\n",
    "\n",
    "# trainer.fit(\n",
    "#     model, optimizer, scheduler,\n",
    "#     start_epoch=0, end_epoch=80\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ish.show_image(batch[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

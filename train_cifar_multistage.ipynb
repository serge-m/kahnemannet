{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/serge-m/pytorch-nn-tools.git@master\n",
    "# !pip install pytorch-nn-tools==0.3.7\n",
    "# !pip install torch_lr_finder==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U albumentations==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Callable, Union\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "from pytorch_nn_tools.visual import ImgShow, tfm_vis_img, UnNormalize_, imagenet_stats\n",
    "from pytorch_nn_tools.train.metrics.processor import mod_name_train, mod_name_val, Marker\n",
    "from pytorch_nn_tools.train.metrics.processor import MetricAggregator, TensorBoardMetricLogger\n",
    "from pytorch_nn_tools.metrics.accuracy import topk_accuracy\n",
    "from pytorch_nn_tools.train.progress import ProgressTracker\n",
    "from pytorch_nn_tools.convert import map_dict\n",
    "from pytorch_nn_tools.train.metrics.history_condition import HistoryCondition\n",
    "from pytorch_nn_tools.devices import to_device\n",
    "import ml_dataset_tools as mdt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer.trainer_io import TrainerIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ish = ImgShow(ax=plt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "\n",
    "batch_size_train, batch_size_val, device = 2, 2, 'cpu'\n",
    "# batch_size_train, batch_size_val, device = 128, 128, 'cuda'\n",
    "\n",
    "data_root_path = Path(\"data/\")\n",
    "data_path = data_root_path.joinpath(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "size_h_w = 224, 224\n",
    "\n",
    "imagenet_stats = dict(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "cifar_stats = dict(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(**cifar_stats),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**cifar_stats),\n",
    "])\n",
    "\n",
    "\n",
    "ds_tr  = datasets.CIFAR10(root=data_root_path, train=True, download=True, transform=transform_train)\n",
    "ds_val = datasets.CIFAR10(root=data_root_path, train=False, download=False, transform=transform_test)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=ds_tr,\n",
    "        batch_size=batch_size_train,\n",
    "        shuffle=True, \n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    \n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=ds_val,\n",
    "        batch_size=batch_size_val,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_images(tb_writer, images, iteration_id):\n",
    "    with torch.no_grad():\n",
    "        vis = images.detach().clone()\n",
    "        for v in vis:\n",
    "            v[:] = UnNormalize_(**cifar_stats)(v)\n",
    "        grid = torchvision.utils.make_grid(vis)\n",
    "        tb_writer.add_image('images', grid, iteration_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.nll_loss(F.log_softmax(output, dim=1), target, reduction='none').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, device, trainer_io: TrainerIO,\n",
    "                continue_training: bool = False):\n",
    "        self.device = device\n",
    "        self.continue_training = continue_training\n",
    "        self.trainer_io = trainer_io\n",
    "        \n",
    "    def fit(self, models, optimizers, schedulers, start_epoch, end_epoch, train_dataloader, val_dataloader):\n",
    "        metric_logger = TensorBoardMetricLogger(self.trainer_io.tb_summary_writer)\n",
    "        models = to_device(models, self.device)\n",
    "        \n",
    "        if self.continue_training:\n",
    "            start_epoch = self.trainer_io.load_last(start_epoch, end_epoch, model, optimizer, scheduler)\n",
    "\n",
    "        progr_train = ProgressTracker()\n",
    "        \n",
    "        for epoch in self.trainer_io.main_progress_bar(range(start_epoch, end_epoch)):\n",
    "            metric_aggregator = MetricAggregator()\n",
    "            self.train_epoch(\n",
    "                train_dataloader, progr_train,\n",
    "                models, optimizers, schedulers,  \n",
    "                metric_proc=mod_name_train+metric_aggregator+metric_logger,\n",
    "                report_step=100,\n",
    "            )\n",
    "            self.validate_epoch(\n",
    "                val_dataloader,\n",
    "                models,  \n",
    "                metric_proc=mod_name_val+metric_aggregator+metric_logger,\n",
    "            )\n",
    "            \n",
    "            aggregated = map_dict(metric_aggregator.aggregate(), key_fn=lambda key: f\"avg.{key}\")\n",
    "            metric_logger({\n",
    "                **aggregated, \n",
    "                **{f\"lr_small_{i}\": lr for i, lr in enumerate(schedulers['small'].get_last_lr())},\n",
    "                **{f\"lr_large_{i}\": lr for i, lr in enumerate(schedulers['large'].get_last_lr())},\n",
    "                Marker.EPOCH: epoch,\n",
    "            })\n",
    "            self.trainer_io.set_main_status_msg(f\"{aggregated}\")\n",
    "#             self.trainer_io.save_checkpoint(aggregated, model, optimizer, scheduler, epoch)\n",
    "            \n",
    "        metric_logger.close()\n",
    "            \n",
    "    def train_epoch(self, data_loader, progr, models, optimizers, schedulers, metric_proc, report_step):\n",
    "        for model in models.values():\n",
    "            model.train()\n",
    "                \n",
    "        for batch in self.trainer_io.secondary_progress_bar(progr.track(data_loader)):\n",
    "            batch = to_device(batch, self.device)\n",
    "            images, target = batch\n",
    "\n",
    "            optimizers['small'].zero_grad()\n",
    "                        \n",
    "            output_small = models['small'](images)\n",
    "            confidence = output_small.log_softmax(axis=1).topk(k=1, dim=1)[0].exp()\n",
    "            min_confidence = output_small.size(1)\n",
    "            confidence = (confidence - min_confidence) / (1. - min_confidence) # scale from 0 to 1\n",
    "            \n",
    "            \n",
    "            optimizers['large'].zero_grad()\n",
    "            output_large = models['large'](images)\n",
    "            loss_large = F.cross_entropy(output_large, target, reduction='none') \n",
    "            loss_large_with_confidence = (loss_large * (1.-confidence.detach())).mean()\n",
    "            loss_large_with_confidence.backward()\n",
    "            optimizers['large'].step()\n",
    "                \n",
    "            # TODO: understand how to apply weight for different items in a batch\n",
    "            loss_small = (\n",
    "                (\n",
    "                    F.cross_entropy(output_small, target, reduction='none') * confidence\n",
    "                ) + \n",
    "                (1-confidence) * loss_large.detach()\n",
    "            ).mean()\n",
    "            loss_small.backward()\n",
    "            \n",
    "            optimizers['small'].step()\n",
    "            schedulers['large'].step()\n",
    "            schedulers['small'].step()\n",
    "            \n",
    "            if progr.cnt_total_iter % report_step == 0:\n",
    "                with torch.no_grad():\n",
    "                    acc_small = topk_accuracy(output_small, target, topk=(1,))[0]\n",
    "                    acc_large = topk_accuracy(output_large, target, topk=(1,))[0]\n",
    "\n",
    "                    metric_proc({\n",
    "                        'loss_small': loss_small, \n",
    "                        'loss_large': loss_large.mean(), \n",
    "                        'loss_large_with_confidence': loss_large_with_confidence, \n",
    "                        'acc_small': acc_small, \n",
    "                        'acc_large': acc_large, \n",
    "                        Marker.ITERATION: progr.cnt_total_iter,\n",
    "                        \n",
    "                    })\n",
    "\n",
    "#             if batch_idx == 0 and tb_writer:\n",
    "#                 publish_images(tb_writer, images, progr.cnt_total_iter)\n",
    "            \n",
    "#         scheduler.step()\n",
    "\n",
    "            \n",
    "\n",
    "    def validate_epoch(self, data_loader, models, metric_proc):\n",
    "        for model in models.values():\n",
    "            model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.trainer_io.secondary_progress_bar(data_loader):\n",
    "                batch = to_device(batch, self.device)\n",
    "                images, target = batch\n",
    "                output_small = models['small'](images)\n",
    "                loss_small = F.cross_entropy(output_small, target)\n",
    "                acc_small = topk_accuracy(output_small, target, topk=(1, ))[0]\n",
    "                metric_proc({'loss_small': loss_small, 'acc_small': acc_small})\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import preact_resnet_from_pytorch_cifar \n",
    "# model = preact_resnet_from_pytorch_cifar.PreActResNet(preact_resnet_from_pytorch_cifar.PreActBlock, [2, 2, 2, 2])\n",
    "def build_model():\n",
    "    return {\n",
    "        'small': preact_resnet_from_pytorch_cifar.PreActResNet(preact_resnet_from_pytorch_cifar.PreActBlock, [1, 1, 1, 1], num_planes=[64,64,64,64]),\n",
    "        'large': preact_resnet_from_pytorch_cifar.PreActResNet(preact_resnet_from_pytorch_cifar.PreActBlock, [2, 2, 2, 2])\n",
    "    }\n",
    "\n",
    "def build_optimizers(model, recommended_lr):\n",
    "    return {\n",
    "        'small': torch.optim.SGD([\n",
    "            {\n",
    "                'params': model['small'].parameters(), \n",
    "                'lr': recommended_lr,\n",
    "                'momentum' :0.9, \n",
    "                'weight_decay': 5e-4\n",
    "            }\n",
    "        ]),\n",
    "        'large': torch.optim.SGD([\n",
    "            {\n",
    "                'params': model['large'].parameters(), \n",
    "                'lr': recommended_lr,\n",
    "                'momentum' :0.9, \n",
    "                'weight_decay': 5e-4\n",
    "            }\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "def build_schedulers(optimizers, recommended_lr, num_epochs, train_dataloader):\n",
    "    return {\n",
    "\n",
    "        'small': torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizers['small'],\n",
    "            max_lr=recommended_lr,\n",
    "            epochs=num_epochs,\n",
    "            steps_per_epoch=len(train_dataloader),\n",
    "            pct_start=0.1,\n",
    "        ),\n",
    "        'large': torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizers['large'],\n",
    "            max_lr=recommended_lr,\n",
    "            epochs=num_epochs,\n",
    "            steps_per_epoch=len(train_dataloader),\n",
    "            pct_start=0.1,\n",
    "        ),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found pretrained results for epoch 49. Loading...\n",
      "loading model from logs/cifar10_preactresnet18pc_small_lr0.1_sgdarr_onecpct0.1_tpu/checkpoints/epoch_00049.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not run 'aten::empty_strided' with arguments from the 'XLA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty_strided' is only available for these backends: [CPU, CUDA, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradNestedTensor, UNKNOWN_TENSOR_TYPE_ID, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:5925 [kernel]\nCUDA: registered at /pytorch/build/aten/src/ATen/RegisterCUDA.cpp:7100 [kernel]\nBackendSelect: registered at /pytorch/build/aten/src/ATen/RegisterBackendSelect.cpp:596 [kernel]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradNestedTensor: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:10499 [kernel]\nAutocast: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:250 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-edc192af0f4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_io_exp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'small'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'small'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschedulers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'small'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/nn/kahnemannet/trainer/trainer_io.py\u001b[0m in \u001b[0;36mload_last\u001b[0;34m(self, start_epoch, end_epoch, model, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlast\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"found pretrained results for epoch {last}. Loading...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlast\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/nn/pytorch-nn-tools/pytorch_nn_tools/train/checkpoint.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, model, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mname_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scheduler'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_module_with_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_module_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_module_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scheduler'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoinpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/nn/pytorch-nn-tools/pytorch_nn_tools/train/checkpoint.py\u001b[0m in \u001b[0;36m_load_module_with_update\u001b[0;34m(self, module_name, dst_module, path)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_module_with_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading {module_name} from {path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mpretrained_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mmodule_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdst_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mpretrained_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpretrained_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule_dict\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/nn/.venv/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/nn/.venv/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/nn/.venv/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_rebuild_xla_tensor\u001b[0;34m(data, dtype, device, requires_grad)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_rebuild_xla_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not run 'aten::empty_strided' with arguments from the 'XLA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty_strided' is only available for these backends: [CPU, CUDA, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradNestedTensor, UNKNOWN_TENSOR_TYPE_ID, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:5925 [kernel]\nCUDA: registered at /pytorch/build/aten/src/ATen/RegisterCUDA.cpp:7100 [kernel]\nBackendSelect: registered at /pytorch/build/aten/src/ATen/RegisterBackendSelect.cpp:596 [kernel]\nNamed: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nAutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradNestedTensor: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nAutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:9273 [autograd kernel]\nTracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_0.cpp:10499 [kernel]\nAutocast: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:250 [backend fallback]\nBatched: registered at /pytorch/aten/src/ATen/BatchingRegistrations.cpp:1016 [backend fallback]\nVmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "trainer_io_exp = TrainerIO(\n",
    "    log_dir=\"./logs/\", experiment_name=f\"cifar10_preactresnet18pc_small_lr0.1_sgdarr_onecpct0.1_tpu\", \n",
    "    checkpoint_condition=HistoryCondition(\n",
    "        'avg.val.acc1', \n",
    "        lambda hist: len(hist) == 1 or hist[-1] > max(hist[:-1])\n",
    "    )\n",
    ")\n",
    "recommended_lr = 0.01\n",
    "num_epochs = 50\n",
    "\n",
    "model = build_model()\n",
    "optimizers = build_optimizers(model, recommended_lr)\n",
    "schedulers = build_schedulers(optimizers, recommended_lr, num_epochs, train_dataloader)\n",
    "\n",
    "\n",
    "start_epoch = trainer_io_exp.load_last(0, 50, model['small'], optimizers['small'], schedulers['small'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_dl = iter(train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(it_dl)\n",
    "images, target = to_device(batch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model['small'](images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1406, -0.1611, -0.0526, -0.2546, -0.1258,  0.2778, -0.0675, -0.1134,\n",
       "          0.0501,  0.3525],\n",
       "        [ 0.1762, -0.1172, -0.0116, -0.3303, -0.0544,  0.2864,  0.0447, -0.0715,\n",
       "          0.0270,  0.2716]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.3525, 0.2864]),\n",
       "indices=tensor([9, 5]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1125, 0.0832, 0.0927, 0.0758, 0.0862, 0.1291, 0.0914, 0.0873, 0.1028,\n",
       "         0.1391],\n",
       "        [0.1149, 0.0857, 0.0952, 0.0692, 0.0912, 0.1282, 0.1007, 0.0897, 0.0989,\n",
       "         0.1264]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = F.log_softmax(logits, dim=1).exp()\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model['small'], input_size=(3, 32, 32), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommended_lr = 0.01\n",
    "num_epochs = 50\n",
    "\n",
    "model = build_model()\n",
    "optimizers = build_optimizers(model, recommended_lr)\n",
    "schedulers = build_schedulers(optimizers, recommended_lr, num_epochs, train_dataloader)\n",
    "\n",
    "trainer_io = TrainerIO(\n",
    "    log_dir=\"./logs/\", experiment_name=f\"cifar10_multistage_1_lr{recommended_lr}_sgdarr_onecpct0.1\", \n",
    "    checkpoint_condition=HistoryCondition(\n",
    "        'avg.val.acc1', \n",
    "        lambda hist: len(hist) == 1 or hist[-1] > max(hist[:-1])\n",
    "    )\n",
    ")\n",
    "\n",
    "trainer = Trainer(device=device, trainer_io=trainer_io, continue_training=False)\n",
    "\n",
    "trainer.fit(\n",
    "    model, optimizers, schedulers,\n",
    "    start_epoch=0, end_epoch=num_epochs,\n",
    "#     train_dataloader=list(islice(train_dataloader, 0, 10)), \n",
    "#     val_dataloader=list(islice(val_dataloader, 0, 10)),\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !rm checkpoints/epoch_00001*\n",
    "# !rm checkpoints/epoch_00002*\n",
    "# !rm checkpoints/epoch_00003*\n",
    "# !rm logs/experiment1/checkpoints/epoch_00004*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
